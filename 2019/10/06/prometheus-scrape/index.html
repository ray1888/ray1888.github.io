<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="description" content="目录        使用的目的 代码实现                      代码版本       基于 prometheus项目的master branch的5a554df0855bf707a8c333c0dd830067d03422cf commit                     使用目的       Prometheus 是一个基">
<meta property="og:type" content="article">
<meta property="og:title" content="Prometheus(1)- 数据抓取源码阅读">
<meta property="og:url" content="http://example.com/2019/10/06/prometheus-scrape/index.html">
<meta property="og:site_name" content="Ray 的藏书阁">
<meta property="og:description" content="目录        使用的目的 代码实现                      代码版本       基于 prometheus项目的master branch的5a554df0855bf707a8c333c0dd830067d03422cf commit                     使用目的       Prometheus 是一个基">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2019/10/06/prometheus-scrape/ScrapeModule.png">
<meta property="article:published_time" content="2019-10-06T07:31:38.000Z">
<meta property="article:modified_time" content="2021-01-22T06:37:38.177Z">
<meta property="article:author" content="Ray Chen">
<meta property="article:tag" content="Prometheus">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2019/10/06/prometheus-scrape/ScrapeModule.png"><title>Prometheus(1)- 数据抓取源码阅读 | Ray 的藏书阁</title><link ref="canonical" href="http://example.com/2019/10/06/prometheus-scrape/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Ray 的藏书阁</div><div class="header-banner-info__subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Prometheus(1)- 数据抓取源码阅读</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-10-06</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-01-22</span></span></div></header><div class="post-body"><!-- ---
layout: post
title: Prometheus数据抓取源码阅读
author: Ray Chan(ray1888)
date: '2019-10-06 15:31:38 +0800'
category: Prometheus
summary: Prometheus Scrape Module
thumbnail: Prometheus.png
--- -->



        <h1 id="目录"   >
          <a href="#目录" class="heading-link"><i class="fas fa-link"></i></a>目录</h1>
      <ol>
<li>使用的目的</li>
<li>代码实现</li>
</ol>

        <h1 id="代码版本"   >
          <a href="#代码版本" class="heading-link"><i class="fas fa-link"></i></a>代码版本</h1>
      <p>基于 prometheus项目的master branch的5a554df0855bf707a8c333c0dd830067d03422cf commit</p>

        <h1 id="使用目的"   >
          <a href="#使用目的" class="heading-link"><i class="fas fa-link"></i></a>使用目的</h1>
      <p>Prometheus 是一个基于Pull模型所进行数据采集的系统，因此，需要在主体项目中有一个抓取数据的模块，而Scrape就是这样的模块。因此这个也是Prometheus的一个主要部分。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">入口代码部分</span><br><span class="line">main.go</span><br><span class="line">&#x2F;&#x2F; line 356</span><br><span class="line">scrapeManager &#x3D; scrape.NewManager(log.With(logger, &quot;component&quot;, &quot;scrape manager&quot;), fanoutStorage)</span><br><span class="line">&#x2F;&#x2F; line 427</span><br><span class="line">scrapeManager.ApplyConfig</span><br><span class="line">&#x2F;&#x2F; line 555</span><br><span class="line">err :&#x3D; scrapeManager.Run(discoveryManagerScrape.SyncCh())</span><br></pre></td></tr></table></div></figure>

        <h1 id="代码实现"   >
          <a href="#代码实现" class="heading-link"><i class="fas fa-link"></i></a>代码实现</h1>
      
        <h2 id="整体的流程图"   >
          <a href="#整体的流程图" class="heading-link"><i class="fas fa-link"></i></a>整体的流程图</h2>
      <!-- ![ServiceDiscoveryModule](/assets/img/posts/prometheus/ScrapeModule.png) -->
<img src="/2019/10/06/prometheus-scrape/ScrapeModule.png" class="" title="ServiceDiscoveryModule">


        <h2 id="目录结构"   >
          <a href="#目录结构" class="heading-link"><i class="fas fa-link"></i></a>目录结构</h2>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r-- 1 ray 197121  2239 9月  30 16:09 helpers_test.go</span><br><span class="line">-rw-r--r-- 1 ray 197121  7543 9月  30 16:09 manager.go   &#x2F;&#x2F;主要的控制的模块Manager</span><br><span class="line">-rw-r--r-- 1 ray 197121 10727 9月  30 16:09 manager_test.go</span><br><span class="line">-rw-r--r-- 1 ray 197121 35749 9月  30 16:09 scrape.go    &#x2F;&#x2F; 主要进行采集的模块</span><br><span class="line">-rw-r--r-- 1 ray 197121 40682 9月  30 16:09 scrape_test.go</span><br><span class="line">-rw-r--r-- 1 ray 197121 11743 9月  30 16:09 target.go    &#x2F;&#x2F;  抓取的公用部分的逻辑</span><br><span class="line">-rw-r--r-- 1 ray 197121  9542 9月  30 16:09 target_test.go</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

        <h2 id="重要数据结构描述"   >
          <a href="#重要数据结构描述" class="heading-link"><i class="fas fa-link"></i></a>重要数据结构描述</h2>
      <p>ScrapeManager 是管理所有抓取的一个抽象</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">type Manager struct &#123;</span><br><span class="line">	logger    log.Logger</span><br><span class="line">	append    Appendable</span><br><span class="line">	graceShut chan struct&#123;&#125;</span><br><span class="line"></span><br><span class="line">	jitterSeed    uint64     &#x2F;&#x2F; Global jitterSeed seed is used to spread scrape workload across HA setup.</span><br><span class="line">	mtxScrape     sync.Mutex &#x2F;&#x2F; Guards the fields below.</span><br><span class="line">	scrapeConfigs map[string]*config.ScrapeConfig</span><br><span class="line">	scrapePools   map[string]*scrapePool</span><br><span class="line">	targetSets    map[string][]*targetgroup.Group</span><br><span class="line"></span><br><span class="line">	triggerReload chan struct&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>ScrapePools 是单个的Job的抓取目标的工作单位</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">appendable Appendable</span><br><span class="line">	logger     log.Logger</span><br><span class="line"></span><br><span class="line">	mtx    sync.RWMutex</span><br><span class="line">	config *config.ScrapeConfig</span><br><span class="line">	client *http.Client</span><br><span class="line">	&#x2F;&#x2F; Targets and loops must always be synchronized to have the same</span><br><span class="line">	&#x2F;&#x2F; set of hashes.</span><br><span class="line">	activeTargets  map[uint64]*Target</span><br><span class="line">	droppedTargets []*Target</span><br><span class="line">	loops          map[uint64]loop</span><br><span class="line">	cancel         context.CancelFunc</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Constructor for new scrape loops. This is settable for testing convenience.</span><br><span class="line">	newLoop func(scrapeLoopOptions) loop</span><br></pre></td></tr></table></div></figure>
<p>loop是单个Target的执行单位，是一个接口。在这里主要使用的是ScrapeLoop的实例</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">type loop interface &#123;</span><br><span class="line">	run(interval, timeout time.Duration, errc chan&lt;- error)</span><br><span class="line">	stop()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type scrapeLoop struct &#123;</span><br><span class="line">	scraper         scraper</span><br><span class="line">	l               log.Logger</span><br><span class="line">	cache           *scrapeCache</span><br><span class="line">	lastScrapeSize  int</span><br><span class="line">	buffers         *pool.Pool</span><br><span class="line">	jitterSeed      uint64</span><br><span class="line">	honorTimestamps bool</span><br><span class="line"></span><br><span class="line">	appender            func() storage.Appender</span><br><span class="line">	sampleMutator       labelsMutator</span><br><span class="line">	reportSampleMutator labelsMutator</span><br><span class="line"></span><br><span class="line">	parentCtx context.Context</span><br><span class="line">	ctx       context.Context</span><br><span class="line">	cancel    func()</span><br><span class="line">	stopped   chan struct&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>scraper接口时具体的执行单位，scrapeLoop也是调用scraper的方法来进行数据的抓取, Prometheus默认使用targetScraper去抓取数据</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">type scraper interface &#123;</span><br><span class="line">	scrape(ctx context.Context, w io.Writer) (string, error) &#x2F;&#x2F; 抓取数据的方法</span><br><span class="line">	report(start time.Time, dur time.Duration, err error)    &#x2F;&#x2F; 上报数据的方法</span><br><span class="line">	offset(interval time.Duration, jitterSeed uint64) time.Duration &#x2F;&#x2F; 记录数据偏移的方法</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type targetScraper struct &#123;</span><br><span class="line">	*Target  &#x2F;&#x2F;包含了report和offset方法</span><br><span class="line"></span><br><span class="line">	client  *http.Client &#x2F;&#x2F;因为Prometheus的exporter是以http接口进行数据的暴露的，所以会有httpclient的结构包含在里面</span><br><span class="line">	req     *http.Request</span><br><span class="line">	timeout time.Duration</span><br><span class="line"></span><br><span class="line">	gzipr *gzip.Reader</span><br><span class="line">	buf   *bufio.Reader</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></div></figure>

        <h2 id="主协程逻辑"   >
          <a href="#主协程逻辑" class="heading-link"><i class="fas fa-link"></i></a>主协程逻辑</h2>
      <p>跟着调用的部分，我们先从初始化后的manager的ApplyConfig方法开始看起。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">func (m *Manager) ApplyConfig(cfg *config.Config) error &#123;</span><br><span class="line">	m.mtxScrape.Lock()</span><br><span class="line">	defer m.mtxScrape.Unlock()</span><br><span class="line"></span><br><span class="line">	c :&#x3D; make(map[string]*config.ScrapeConfig)</span><br><span class="line">    </span><br><span class="line">	for _, scfg :&#x3D; range cfg.ScrapeConfigs &#123;</span><br><span class="line">		c[scfg.JobName] &#x3D; scfg</span><br><span class="line">	&#125;</span><br><span class="line">	m.scrapeConfigs &#x3D; c</span><br><span class="line">    &#x2F;&#x2F; 使用全局配置来生成一个集群内不重复的seed</span><br><span class="line">	if err :&#x3D; m.setJitterSeed(cfg.GlobalConfig.ExternalLabels); err !&#x3D; nil &#123;</span><br><span class="line">		return err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Cleanup and reload pool if the configuration has changed.</span><br><span class="line">	var failed bool</span><br><span class="line">    &#x2F;&#x2F; 根据解析出来的配置生成对应的ScrapePool， 如果有并且数据没有改变的话，那就不进行操作，否则</span><br><span class="line">	for name, sp :&#x3D; range m.scrapePools &#123;</span><br><span class="line">		if cfg, ok :&#x3D; m.scrapeConfigs[name]; !ok &#123;</span><br><span class="line">			sp.stop()</span><br><span class="line">			delete(m.scrapePools, name)</span><br><span class="line">		&#125; else if !reflect.DeepEqual(sp.config, cfg) &#123;</span><br><span class="line">			err :&#x3D; sp.reload(cfg)</span><br><span class="line">			if err !&#x3D; nil &#123;</span><br><span class="line">				level.Error(m.logger).Log(&quot;msg&quot;, &quot;error reloading scrape pool&quot;, &quot;err&quot;, err, &quot;scrape_pool&quot;, name)</span><br><span class="line">				failed &#x3D; true</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if failed &#123;</span><br><span class="line">		return errors.New(&quot;failed to apply the new configuration&quot;)</span><br><span class="line">	&#125;</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">func (sp *scrapePool) reload(cfg *config.ScrapeConfig) error &#123;</span><br><span class="line">	targetScrapePoolReloads.Inc()</span><br><span class="line">	start :&#x3D; time.Now()</span><br><span class="line"></span><br><span class="line">	sp.mtx.Lock()</span><br><span class="line">	defer sp.mtx.Unlock()</span><br><span class="line"></span><br><span class="line">	client, err :&#x3D; config_util.NewClientFromConfig(cfg.HTTPClientConfig, cfg.JobName, false)</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		targetScrapePoolReloadsFailed.Inc()</span><br><span class="line">		return errors.Wrap(err, &quot;error creating HTTP client&quot;)</span><br><span class="line">	&#125;</span><br><span class="line">	sp.config &#x3D; cfg</span><br><span class="line">	oldClient :&#x3D; sp.client</span><br><span class="line">	sp.client &#x3D; client</span><br><span class="line"></span><br><span class="line">	var (</span><br><span class="line">		wg              sync.WaitGroup</span><br><span class="line">		interval        &#x3D; time.Duration(sp.config.ScrapeInterval)</span><br><span class="line">		timeout         &#x3D; time.Duration(sp.config.ScrapeTimeout)</span><br><span class="line">		limit           &#x3D; int(sp.config.SampleLimit)</span><br><span class="line">		honorLabels     &#x3D; sp.config.HonorLabels</span><br><span class="line">		honorTimestamps &#x3D; sp.config.HonorTimestamps</span><br><span class="line">		mrc             &#x3D; sp.config.MetricRelabelConfigs</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	for fp, oldLoop :&#x3D; range sp.loops &#123;</span><br><span class="line">		var (</span><br><span class="line">			t       &#x3D; sp.activeTargets[fp]</span><br><span class="line">			s       &#x3D; &amp;targetScraper&#123;Target: t, client: sp.client, timeout: timeout&#125;</span><br><span class="line">			newLoop &#x3D; sp.newLoop(scrapeLoopOptions&#123;</span><br><span class="line">				target:          t,</span><br><span class="line">				scraper:         s,</span><br><span class="line">				limit:           limit,</span><br><span class="line">				honorLabels:     honorLabels,</span><br><span class="line">				honorTimestamps: honorTimestamps,</span><br><span class="line">				mrc:             mrc,</span><br><span class="line">			&#125;)</span><br><span class="line">		)</span><br><span class="line">		wg.Add(1)</span><br><span class="line"></span><br><span class="line">		go func(oldLoop, newLoop loop) &#123;</span><br><span class="line">			oldLoop.stop()</span><br><span class="line">			wg.Done()</span><br><span class="line"></span><br><span class="line">			go newLoop.run(interval, timeout, nil)</span><br><span class="line">		&#125;(oldLoop, newLoop)</span><br><span class="line"></span><br><span class="line">		sp.loops[fp] &#x3D; newLoop</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	wg.Wait()</span><br><span class="line">	oldClient.CloseIdleConnections()</span><br><span class="line">	targetReloadIntervalLength.WithLabelValues(interval.String()).Observe(</span><br><span class="line">		time.Since(start).Seconds(),</span><br><span class="line">	)</span><br><span class="line">	return nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<ol>
<li>把解析好的配置，遍历，变为一个jobName为key，配置值为Value的map</li>
<li>对比自己的配置，如果之前已经存在但是配置发生变动的，则去reload scraper pool的配置。</li>
<li>（Reload） 如果需要reload配置的情况下，会重新生成scrapePool后，派生多一个线程去执行scraperPool.Sync()，知道Manager的targetSet被遍历完为止。Sync方法的内容会后面进行详细讲解。</li>
</ol>
<p>Run()方法</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">func (m *Manager) Run(tsets &lt;-chan map[string][]*targetgroup.Group) error &#123;</span><br><span class="line">	go m.reloader()</span><br><span class="line">	for &#123;</span><br><span class="line">		select &#123;</span><br><span class="line">		case ts :&#x3D; &lt;-tsets:</span><br><span class="line">			m.updateTsets(ts)</span><br><span class="line"></span><br><span class="line">			select &#123;</span><br><span class="line">			case m.triggerReload &lt;- struct&#123;&#125;&#123;&#125;:</span><br><span class="line">			default:</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		case &lt;-m.graceShut:</span><br><span class="line">			return nil</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func (m *Manager) reloader() &#123;</span><br><span class="line">	ticker :&#x3D; time.NewTicker(5 * time.Second)</span><br><span class="line">	defer ticker.Stop()</span><br><span class="line"></span><br><span class="line">	for &#123;</span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-m.graceShut:</span><br><span class="line">			return</span><br><span class="line">		case &lt;-ticker.C:</span><br><span class="line">			select &#123;</span><br><span class="line">			case &lt;-m.triggerReload:</span><br><span class="line">				m.reload()</span><br><span class="line">			case &lt;-m.graceShut:</span><br><span class="line">				return</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>功能：</p>
<ol>
<li>等待从Main.go中传入的discoveryManager的SyncCh是否有变动，如果有变动，更新Targetset。</li>
<li>派生出了一个Reloader协程，Reloader协程会定时检查是否有关闭的信号或者Reload信号（triggerReload channel,就是外部给与主协程的刺激产生的二级信号），如果有，则执行reload操作。</li>
</ol>

        <h2 id="子协程逻辑"   >
          <a href="#子协程逻辑" class="heading-link"><i class="fas fa-link"></i></a>子协程逻辑</h2>
      
        <h3 id="ScraperPool"   >
          <a href="#ScraperPool" class="heading-link"><i class="fas fa-link"></i></a>ScraperPool</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Sync converts target groups into actual scrape targets and synchronizes</span><br><span class="line">&#x2F;&#x2F; the currently running scraper with the resulting set and returns all scraped and dropped targets.</span><br><span class="line">func (sp *scrapePool) Sync(tgs []*targetgroup.Group) &#123;</span><br><span class="line">	start :&#x3D; time.Now()</span><br><span class="line"></span><br><span class="line">	var all []*Target</span><br><span class="line">	sp.mtx.Lock()</span><br><span class="line">	sp.droppedTargets &#x3D; []*Target&#123;&#125;</span><br><span class="line">	for _, tg :&#x3D; range tgs &#123;</span><br><span class="line">		targets, err :&#x3D; targetsFromGroup(tg, sp.config)</span><br><span class="line">		if err !&#x3D; nil &#123;</span><br><span class="line">			level.Error(sp.logger).Log(&quot;msg&quot;, &quot;creating targets failed&quot;, &quot;err&quot;, err)</span><br><span class="line">			continue</span><br><span class="line">		&#125;</span><br><span class="line">		for _, t :&#x3D; range targets &#123;</span><br><span class="line">			if t.Labels().Len() &gt; 0 &#123;</span><br><span class="line">				all &#x3D; append(all, t)</span><br><span class="line">			&#125; else if t.DiscoveredLabels().Len() &gt; 0 &#123;</span><br><span class="line">				sp.droppedTargets &#x3D; append(sp.droppedTargets, t)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	sp.mtx.Unlock()</span><br><span class="line">	sp.sync(all)</span><br><span class="line"></span><br><span class="line">	targetSyncIntervalLength.WithLabelValues(sp.config.JobName).Observe(</span><br><span class="line">		time.Since(start).Seconds(),</span><br><span class="line">	)</span><br><span class="line">	targetScrapePoolSyncsCounter.WithLabelValues(sp.config.JobName).Inc()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>Sync函数是一个对外暴露函数的接口：</p>
<ol>
<li>把配置解析出来的target结构化。</li>
<li>调用内部方法sync()来进行数据抓取的执行</li>
<li>一些计数器添加计数</li>
</ol>
<p>值得注意的是Append方法，是一个封装了的方法，是同是进行对变量的修改，并且包含了采集到的数据持久化的操作。</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; sync takes a list of potentially duplicated targets, deduplicates them, starts</span><br><span class="line">&#x2F;&#x2F; scrape loops for new targets, and stops scrape loops for disappeared targets.</span><br><span class="line">&#x2F;&#x2F; It returns after all stopped scrape loops terminated.</span><br><span class="line">func (sp *scrapePool) sync(targets []*Target) &#123;</span><br><span class="line">	sp.mtx.Lock()</span><br><span class="line">	defer sp.mtx.Unlock()</span><br><span class="line"></span><br><span class="line">	var (</span><br><span class="line">		uniqueTargets   &#x3D; map[uint64]struct&#123;&#125;&#123;&#125;</span><br><span class="line">		interval        &#x3D; time.Duration(sp.config.ScrapeInterval)</span><br><span class="line">		timeout         &#x3D; time.Duration(sp.config.ScrapeTimeout)</span><br><span class="line">		limit           &#x3D; int(sp.config.SampleLimit)</span><br><span class="line">		honorLabels     &#x3D; sp.config.HonorLabels</span><br><span class="line">		honorTimestamps &#x3D; sp.config.HonorTimestamps</span><br><span class="line">		mrc             &#x3D; sp.config.MetricRelabelConfigs</span><br><span class="line">	)</span><br><span class="line"></span><br><span class="line">	for _, t :&#x3D; range targets &#123;</span><br><span class="line">		t :&#x3D; t</span><br><span class="line">		hash :&#x3D; t.hash()</span><br><span class="line">		uniqueTargets[hash] &#x3D; struct&#123;&#125;&#123;&#125;</span><br><span class="line"></span><br><span class="line">		if _, ok :&#x3D; sp.activeTargets[hash]; !ok &#123;</span><br><span class="line">			s :&#x3D; &amp;targetScraper&#123;Target: t, client: sp.client, timeout: timeout&#125;</span><br><span class="line">			l :&#x3D; sp.newLoop(scrapeLoopOptions&#123;</span><br><span class="line">				target:          t,</span><br><span class="line">				scraper:         s,</span><br><span class="line">				limit:           limit,</span><br><span class="line">				honorLabels:     honorLabels,</span><br><span class="line">				honorTimestamps: honorTimestamps,</span><br><span class="line">				mrc:             mrc,</span><br><span class="line">			&#125;)</span><br><span class="line"></span><br><span class="line">			sp.activeTargets[hash] &#x3D; t</span><br><span class="line">			sp.loops[hash] &#x3D; l</span><br><span class="line"></span><br><span class="line">			go l.run(interval, timeout, nil)</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			&#x2F;&#x2F; Need to keep the most updated labels information</span><br><span class="line">			&#x2F;&#x2F; for displaying it in the Service Discovery web page.</span><br><span class="line">			sp.activeTargets[hash].SetDiscoveredLabels(t.DiscoveredLabels())</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	var wg sync.WaitGroup</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Stop and remove old targets and scraper loops.</span><br><span class="line">	for hash :&#x3D; range sp.activeTargets &#123;</span><br><span class="line">		if _, ok :&#x3D; uniqueTargets[hash]; !ok &#123;</span><br><span class="line">			wg.Add(1)</span><br><span class="line">			go func(l loop) &#123;</span><br><span class="line"></span><br><span class="line">				l.stop()</span><br><span class="line"></span><br><span class="line">				wg.Done()</span><br><span class="line">			&#125;(sp.loops[hash])</span><br><span class="line"></span><br><span class="line">			delete(sp.loops, hash)</span><br><span class="line">			delete(sp.activeTargets, hash)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; Wait for all potentially stopped scrapers to terminate.</span><br><span class="line">	&#x2F;&#x2F; This covers the case of flapping targets. If the server is under high load, a new scraper</span><br><span class="line">	&#x2F;&#x2F; may be active and tries to insert. The old scraper that didn&#39;t terminate yet could still</span><br><span class="line">	&#x2F;&#x2F; be inserting a previous sample set.</span><br><span class="line">	wg.Wait()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>主要逻辑：</p>
<ol>
<li>把传入的Target列表进行遍历<br>1.1 如果target不在active的map中， 生成targetScraper，然后把targetScraper放入Loop里面，调用Loop.run()在协程中进行逻辑<br>1.2 否则， 会先删除旧的协程，然后重新生成协程。</li>
</ol>

        <h3 id="ScraperLoop"   >
          <a href="#ScraperLoop" class="heading-link"><i class="fas fa-link"></i></a>ScraperLoop</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">func (sl *scrapeLoop) run(interval, timeout time.Duration, errc chan&lt;- error) &#123;</span><br><span class="line">	select &#123;</span><br><span class="line">	case &lt;-time.After(sl.scraper.offset(interval, sl.jitterSeed)):</span><br><span class="line">		&#x2F;&#x2F; Continue after a scraping offset.</span><br><span class="line">	case &lt;-sl.ctx.Done():</span><br><span class="line">		close(sl.stopped)</span><br><span class="line">		return</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	var last time.Time</span><br><span class="line"></span><br><span class="line">	ticker :&#x3D; time.NewTicker(interval)</span><br><span class="line">	defer ticker.Stop()</span><br><span class="line"></span><br><span class="line">mainLoop:</span><br><span class="line">	for &#123;</span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-sl.parentCtx.Done():</span><br><span class="line">			close(sl.stopped)</span><br><span class="line">			return</span><br><span class="line">		case &lt;-sl.ctx.Done():</span><br><span class="line">			break mainLoop</span><br><span class="line">		default:</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		var (</span><br><span class="line">			start             &#x3D; time.Now()</span><br><span class="line">			scrapeCtx, cancel &#x3D; context.WithTimeout(sl.ctx, timeout)</span><br><span class="line">		)</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; Only record after the first scrape.</span><br><span class="line">		if !last.IsZero() &#123;</span><br><span class="line">			targetIntervalLength.WithLabelValues(interval.String()).Observe(</span><br><span class="line">				time.Since(last).Seconds(),</span><br><span class="line">			)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		b :&#x3D; sl.buffers.Get(sl.lastScrapeSize).([]byte)</span><br><span class="line">		buf :&#x3D; bytes.NewBuffer(b)</span><br><span class="line"></span><br><span class="line">		contentType, scrapeErr :&#x3D; sl.scraper.scrape(scrapeCtx, buf)</span><br><span class="line">		cancel()</span><br><span class="line"></span><br><span class="line">		if scrapeErr &#x3D;&#x3D; nil &#123;</span><br><span class="line">			b &#x3D; buf.Bytes()</span><br><span class="line">			&#x2F;&#x2F; NOTE: There were issues with misbehaving clients in the past</span><br><span class="line">			&#x2F;&#x2F; that occasionally returned empty results. We don&#39;t want those</span><br><span class="line">			&#x2F;&#x2F; to falsely reset our buffer size.</span><br><span class="line">			if len(b) &gt; 0 &#123;</span><br><span class="line">				sl.lastScrapeSize &#x3D; len(b)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			level.Debug(sl.l).Log(&quot;msg&quot;, &quot;Scrape failed&quot;, &quot;err&quot;, scrapeErr.Error())</span><br><span class="line">			if errc !&#x3D; nil &#123;</span><br><span class="line">				errc &lt;- scrapeErr</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		&#x2F;&#x2F; A failed scrape is the same as an empty scrape,</span><br><span class="line">		&#x2F;&#x2F; we still call sl.append to trigger stale markers.</span><br><span class="line">		total, added, seriesAdded, appErr :&#x3D; sl.append(b, contentType, start)</span><br><span class="line">		if appErr !&#x3D; nil &#123;</span><br><span class="line">			level.Warn(sl.l).Log(&quot;msg&quot;, &quot;append failed&quot;, &quot;err&quot;, appErr)</span><br><span class="line">			&#x2F;&#x2F; The append failed, probably due to a parse error or sample limit.</span><br><span class="line">			&#x2F;&#x2F; Call sl.append again with an empty scrape to trigger stale markers.</span><br><span class="line">			if _, _, _, err :&#x3D; sl.append([]byte&#123;&#125;, &quot;&quot;, start); err !&#x3D; nil &#123;</span><br><span class="line">				level.Warn(sl.l).Log(&quot;msg&quot;, &quot;append failed&quot;, &quot;err&quot;, err)</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		sl.buffers.Put(b)</span><br><span class="line"></span><br><span class="line">		if scrapeErr &#x3D;&#x3D; nil &#123;</span><br><span class="line">			scrapeErr &#x3D; appErr</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if err :&#x3D; sl.report(start, time.Since(start), total, added, seriesAdded, scrapeErr); err !&#x3D; nil &#123;</span><br><span class="line">			level.Warn(sl.l).Log(&quot;msg&quot;, &quot;appending scrape report failed&quot;, &quot;err&quot;, err)</span><br><span class="line">		&#125;</span><br><span class="line">		last &#x3D; start</span><br><span class="line"></span><br><span class="line">		select &#123;</span><br><span class="line">		case &lt;-sl.parentCtx.Done():</span><br><span class="line">			close(sl.stopped)</span><br><span class="line">			return</span><br><span class="line">		case &lt;-sl.ctx.Done():</span><br><span class="line">			break mainLoop</span><br><span class="line">		case &lt;-ticker.C:</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	close(sl.stopped)</span><br><span class="line"></span><br><span class="line">	sl.endOfRunStaleness(last, ticker, interval)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>
<p>ScraperLoop是单个Target进行获取的执行单位，协程使用死循环进行占用，然后调用scraper接口的Scrape方法去抓取数据，并且调用Stroage模块的Appender的接口金属数据的持久化，然后继续定时休眠的过程。我们需要更加具体的看一下实例Scraper的Scrape方法。</p>
<p>ScraperLoop把Scraper抽象出来的三个接口都进行了调用：</p>
<ol>
<li>开始部分的Select代码段中的Offset是用于控制第一次执行的时候等待的间隔</li>
<li>Scrape方法就是直接进行数据的抓取，下面有详细解析</li>
<li>report方法，修改Scraper中Target自己保存的状态。</li>
</ol>

        <h3 id="TargerScraper"   >
          <a href="#TargerScraper" class="heading-link"><i class="fas fa-link"></i></a>TargerScraper</h3>
      <figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">func (s *targetScraper) scrape(ctx context.Context, w io.Writer) (string, error) &#123;</span><br><span class="line">	if s.req &#x3D;&#x3D; nil &#123;</span><br><span class="line">		req, err :&#x3D; http.NewRequest(&quot;GET&quot;, s.URL().String(), nil)</span><br><span class="line">		if err !&#x3D; nil &#123;</span><br><span class="line">			return &quot;&quot;, err</span><br><span class="line">		&#125;</span><br><span class="line">		req.Header.Add(&quot;Accept&quot;, acceptHeader)</span><br><span class="line">		req.Header.Add(&quot;Accept-Encoding&quot;, &quot;gzip&quot;)</span><br><span class="line">		req.Header.Set(&quot;User-Agent&quot;, userAgentHeader)</span><br><span class="line">		req.Header.Set(&quot;X-Prometheus-Scrape-Timeout-Seconds&quot;, fmt.Sprintf(&quot;%f&quot;, s.timeout.Seconds()))</span><br><span class="line"></span><br><span class="line">		s.req &#x3D; req</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	resp, err :&#x3D; s.client.Do(s.req.WithContext(ctx))</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	defer func() &#123;</span><br><span class="line">		io.Copy(ioutil.Discard, resp.Body)</span><br><span class="line">		resp.Body.Close()</span><br><span class="line">	&#125;()</span><br><span class="line"></span><br><span class="line">	if resp.StatusCode !&#x3D; http.StatusOK &#123;</span><br><span class="line">		return &quot;&quot;, errors.Errorf(&quot;server returned HTTP status %s&quot;, resp.Status)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if resp.Header.Get(&quot;Content-Encoding&quot;) !&#x3D; &quot;gzip&quot; &#123;</span><br><span class="line">		_, err &#x3D; io.Copy(w, resp.Body)</span><br><span class="line">		if err !&#x3D; nil &#123;</span><br><span class="line">			return &quot;&quot;, err</span><br><span class="line">		&#125;</span><br><span class="line">		return resp.Header.Get(&quot;Content-Type&quot;), nil</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if s.gzipr &#x3D;&#x3D; nil &#123;</span><br><span class="line">		s.buf &#x3D; bufio.NewReader(resp.Body)</span><br><span class="line">		s.gzipr, err &#x3D; gzip.NewReader(s.buf)</span><br><span class="line">		if err !&#x3D; nil &#123;</span><br><span class="line">			return &quot;&quot;, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		s.buf.Reset(resp.Body)</span><br><span class="line">		if err &#x3D; s.gzipr.Reset(s.buf); err !&#x3D; nil &#123;</span><br><span class="line">			return &quot;&quot;, err</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	_, err &#x3D; io.Copy(w, s.gzipr)</span><br><span class="line">	s.gzipr.Close()</span><br><span class="line">	if err !&#x3D; nil &#123;</span><br><span class="line">		return &quot;&quot;, err</span><br><span class="line">	&#125;</span><br><span class="line">	return resp.Header.Get(&quot;Content-Type&quot;), nil</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></div></figure>
<p>Scrape方法是使用HttpClient进行对target url 的数据抓取，抓取的内容在context中进行传递，得到返回后，继续解析，返回给ScraperLoop的Run方法使用。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://example.com">Ray Chen</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://example.com/2019/10/06/prometheus-scrape/">http://example.com/2019/10/06/prometheus-scrape/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/Prometheus/">Prometheus</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2020/01/21/git-rebase/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">git-rebase</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2019/10/05/prometheus-service-discovery/"><span class="paginator-prev__text">Prometheus服务发现源码阅读</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">
          目录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%89%88%E6%9C%AC"><span class="toc-number">2.</span> <span class="toc-text">
          代码版本</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9B%AE%E7%9A%84"><span class="toc-number">3.</span> <span class="toc-text">
          使用目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">
          代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-number">4.1.</span> <span class="toc-text">
          整体的流程图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-number">4.2.</span> <span class="toc-text">
          目录结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%8F%8F%E8%BF%B0"><span class="toc-number">4.3.</span> <span class="toc-text">
          重要数据结构描述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E5%8D%8F%E7%A8%8B%E9%80%BB%E8%BE%91"><span class="toc-number">4.4.</span> <span class="toc-text">
          主协程逻辑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%90%E5%8D%8F%E7%A8%8B%E9%80%BB%E8%BE%91"><span class="toc-number">4.5.</span> <span class="toc-text">
          子协程逻辑</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ScraperPool"><span class="toc-number">4.5.1.</span> <span class="toc-text">
          ScraperPool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ScraperLoop"><span class="toc-number">4.5.2.</span> <span class="toc-text">
          ScraperLoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TargerScraper"><span class="toc-number">4.5.3.</span> <span class="toc-text">
          TargerScraper</span></a></li></ol></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/header.jpeg" alt="avatar"></div><p class="sidebar-ov-author__text">Believe in Goods in People</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">0</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">6</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Ray Chen</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script></body></html>